{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92b885dd147dac19bd0a33db3cd0da100bd5bc23",
    "id": "cnazc1ZMql1F"
   },
   "source": [
    "# Twitter Sentiment Analysis\n",
    "This pipeline is based around: https://www.kaggle.com/code/paoloripamonti/twitter-sentiment-analysis <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "e9UKei7UB0nM"
   },
   "outputs": [],
   "source": [
    "#!sudo pip3 uninstall gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "70282bce8b42a51e4d44f2c7d85c4ca9567b0fd4",
    "id": "vrwfl9Cdql1J"
   },
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip3 install keras==2.7.0rc2\n",
    "#!pip install -U scikit-learn scipy statsmodels\n",
    "#!pip3 install torchtext\n",
    "#!pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "303e72966af732ddef0bd8108a321095314e44af",
    "id": "O8lxGEQeql1L"
   },
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Pre-trained embeddings\n",
    "import torchtext.vocab as vocab\n",
    "import torch\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Keras\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from keras import utils\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "\n",
    "# Word2vec\n",
    "import gensim\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "# Drive things\n",
    "#from google.colab import drive\n",
    "\n",
    "# Oversampling \n",
    "from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "35e1a89dead5fd160e4c9a024a21d2e569fc89ff",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCMEub4rql1M",
    "outputId": "5ccc5a30-97b6-4821-9795-99a642bf4b9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christianrasmussen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "#drive.mount('/content/myDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFKU8hECsTyk",
    "outputId": "88fecbe2-77df-44ac-9212-a58d520bdbba"
   },
   "outputs": [],
   "source": [
    "#cd myDrive/MyDrive/Colab Notebooks/project/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "0tcdTWw1KLiF"
   },
   "outputs": [],
   "source": [
    "#use_tpu = True #@param {type:\"boolean\"}\n",
    "#\n",
    "#if use_tpu:\n",
    "#    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
    "#\n",
    "#if 'COLAB_TPU_ADDR' in os.environ:\n",
    "#  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
    "#else:\n",
    "#  TF_MASTER=''\n",
    "## Model specific parameters\n",
    "#\n",
    "## TPU address\n",
    "#tpu_address = TF_MASTER\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8b01a07df001e4abcc745900336c4db06e455f3",
    "id": "Ad0YBcTeql1N"
   },
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter to change depending on whether or not to use a pretrained model.\n",
    "# Preffered if running on local hardware btw. \n",
    "pretrained_model = True\n",
    "\n",
    "#Choose which domain to be source domain\n",
    "source = 'NB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "180f0dd2a95419e4602b5c0229822b0111c826f6",
    "id": "LBBW3l5_ql1N"
   },
   "outputs": [],
   "source": [
    "# Reproducability\n",
    "seed = 1234\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# TEXT CLENAING\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "# WORD2VEC \n",
    "embedding_dim = 300\n",
    "\n",
    "# model settings\n",
    "SEQUENCE_LENGTH = 300\n",
    "epochs = 30\n",
    "batch_size = 75\n",
    "\n",
    "# EXPORT\n",
    "KERAS_MODEL = f\"{source}-ros-e30-bs75.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1c3beecc618be68480b3d4f0de08d9d863da1dc1",
    "id": "G8KhALr7ql1O"
   },
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "bba8f91cd70de4f5ea0fb0870ae2029b6e3dcc24",
    "id": "GUk7G5rwql1Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8074\n",
      "Dataset size: 1000\n",
      "Dataset size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Reading needed datasets\n",
    "source_domain = {'LR': '../data_synthesising/synthesized_data/LR_data.csv',\n",
    "                  'NB' : '../data_synthesising/synthesized_data/NB_data.csv',\n",
    "                  'JS' : '../data_synthesising/synthesized_data/JS_data.csv',\n",
    "                  'EA' : '../data_synthesising/synthesized_data/equal_amounts.csv'}\n",
    "validation_domain = '../data_synthesising/val_data/kindle_reviews.csv'\n",
    "test_domain = '../data_synthesising/test_data/amazon_reviews.csv'\n",
    "\n",
    "# Path to pretrained_model\n",
    "model_path = f'../pretrained_models/{source}-ROS-E30-BS75.h5'\n",
    "\n",
    "train = pd.read_csv(source_domain[source]) # Change string to swap out source domain\n",
    "test = pd.read_csv(test_domain) # Kindle review domain\n",
    "val = pd.read_csv(validation_domain)\n",
    "\n",
    "\n",
    "# Oversampling data to match weight amounts\n",
    "ros = RandomOverSampler(random_state=seed)\n",
    "x, y = ros.fit_resample(train, train['label'])\n",
    "\n",
    "train = x\n",
    "print(\"Dataset size:\", len(train))\n",
    "print(\"Dataset size:\", len(val))\n",
    "print(\"Dataset size:\", len(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4329b1573518b03e497213efa7676220734ebb4b",
    "id": "nmOKrLV5ql1T"
   },
   "source": [
    "### Pre-Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "8aeee8b7b9ea11b749c7f91cd4787a7b50ed1a91",
    "id": "aqJ0z5XUql1T"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "649ebcb97969b9ac4301138783704bb3d7846a49",
    "id": "CEVubo8xql1T"
   },
   "outputs": [],
   "source": [
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "f7f3e77ab9291d14687c49e71ba9b2b1e3323432",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyqSJAcgql1U",
    "outputId": "d3b8e237-9109-42f2-8808-0733102e271d"
   },
   "outputs": [],
   "source": [
    "train.text = train.text.apply(lambda x: preprocess(x))\n",
    "test.text = test.text.apply(lambda x: preprocess(x))\n",
    "val.text = val.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5f9714a8507409bbe780eebf2855a33e8e6ba37",
    "id": "GdwoxXyOql1U"
   },
   "source": [
    "### Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "d2b1179c968e3f3910c790ecf0c5b2cbb34b0e68",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cOu06_Gql1U",
    "outputId": "02c93160-6d40-457c-ed15-4c4a7275472a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN size: 8074\n"
     ]
    }
   ],
   "source": [
    "df_train = train\n",
    "df_test = test\n",
    "df_val = val\n",
    "print(\"TRAIN size:\", len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f08a28aab2c3d16d8b9681a7d5d07587153a1cd6",
    "id": "FheBcdmKql1V"
   },
   "source": [
    "### Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8lEfaebcNo0m",
    "outputId": "c850b227-93db-4c13-d67c-0fdf60937080"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 02:49:27,135 : INFO : Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n"
     ]
    }
   ],
   "source": [
    "# Pretrained attempt: \n",
    "glove = vocab.GloVe('6B', dim = embedding_dim)\n",
    "def get_word(word):\n",
    "    return glove.vectors[glove.stoi[word]]\n",
    "def closest(vec, n=10):\n",
    "    \"\"\"\n",
    "    Find the closest words for a given vector\n",
    "    \"\"\"\n",
    "    all_dists = [(w, torch.dist(vec, get_word(w))) for w in glove.itos]\n",
    "    return sorted(all_dists, key=lambda t: t[1])[:n]\n",
    "def print_tuples(tuples):\n",
    "    for tuple in tuples:\n",
    "        print('(%.4f) %s' % (tuple[1], tuple[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e13563644468037258598637b49373ca96b9b879",
    "id": "l9d_Zdxoql1W"
   },
   "source": [
    "### Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "6852bc709a7cd20173cbeeb218505078f8f37c57",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXwiX-Vfql1X",
    "outputId": "c66ec71e-6dd4-4cca-8884-51529e76b669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Total words 29320\n"
     ]
    }
   ],
   "source": [
    "print(type(df_train.text))\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train.text)\n",
    "tokenizer.fit_on_texts(df_test.text)\n",
    "tokenizer.fit_on_texts(df_val.text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Total words\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "45de439df3015030c71f84c2d170346936a1d68f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uedNHN6ql1X",
    "outputId": "3222cd06-e261-445e-e175-b2484cac42be"
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)\n",
    "y_train = df_train['label']\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)\n",
    "y_test = df_test['label']\n",
    "x_val =  pad_sequences(tokenizer.texts_to_sequences(df_val.text), maxlen=SEQUENCE_LENGTH)\n",
    "y_val = df_val['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "233c0ea94055a03e2e7df3e2a13d036ec963484f",
    "id": "Oa1AiZkQql1Y"
   },
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "9ab488374b59e3f30f8b1ea92767d853c4846bac",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRdGNkdkql1Y",
    "outputId": "7f286a2b-8e00-4314-ca03-a3c3314fb9ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29320, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_matrix[i] = get_word(word)\n",
    "    except:\n",
    "        pass\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "833279d91e4286065968237fb5f2a0c2dd4d246c",
    "id": "AGHV0dNMql1Z"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbwW0pDxFQyx",
    "outputId": "315efeef-b420-4950-dd61-374e11eeab6a"
   },
   "outputs": [],
   "source": [
    "#resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)\n",
    "#tf.config.experimental_connect_to_cluster(resolver)\n",
    "#tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "#strategy = tf.distribute.experimental.TPUStrategy(resolver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8d0873633dd49179c8cae17377641b97d323ef3b",
    "id": "lMtvUl5Lql1a"
   },
   "source": [
    "### Training and building our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_uuid": "2b659d390c6577dc5cdb6b6297934279b4e801d5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNRcBa-hql1a",
    "outputId": "7e53ab19-a134-480a-f769-27142658b32d"
   },
   "outputs": [],
   "source": [
    "if not pretrained_model:\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "    callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
    "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        #validation_split=0.1, #Untab to do validation splits on source domain\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks)\n",
    "else:\n",
    "    model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "267258196d96796ac69a7b8c466314bcf5d6ee42",
    "id": "wRQjvUi_ql1a"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "_uuid": "98ecd8f1b8b74594c3ea775dd68a094e92458022",
    "id": "MNYX7XKTql1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 100ms/step - loss: 0.9312 - accuracy: 0.7830\n",
      "20/20 [==============================] - 2s 98ms/step - loss: 1.3022 - accuracy: 0.7230\n",
      "\n",
      "VALIDATION ACCURACY: 0.7829999923706055\n",
      "VALIDATION LOSS: 0.9311715364456177\n",
      "\n",
      "TEST ACCURACY: 0.7229999899864197\n",
      "TEST LOSS: 1.3022006750106812\n"
     ]
    }
   ],
   "source": [
    "score1 = model.evaluate(x_val, y_val, batch_size=50)\n",
    "score = model.evaluate(x_test, y_test, batch_size=50)\n",
    "print()\n",
    "print(\"VALIDATION ACCURACY:\",score1[1])\n",
    "print(\"VALIDATION LOSS:\",score1[0])\n",
    "print()\n",
    "print(\"TEST ACCURACY:\",score[1])\n",
    "print(\"TEST LOSS:\",score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_uuid": "40c72cd1e9d6c4fd799cbba7c813765ac4039dfc",
    "id": "5FyqZQiXql1a"
   },
   "outputs": [],
   "source": [
    "if not pretrained_model:\n",
    "    for i in history.history:\n",
    "        print(i)\n",
    "    acc = history.history['accuracy']\n",
    "    #val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    #val_loss = history.history['val_loss']\n",
    "     \n",
    "    epochs = range(len(acc))\n",
    "     \n",
    "    plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "    plt.title('Training accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6bdfc0f6a6af5bebc0271d83dd7432c91001409b",
    "id": "O53LMN-Uql1a"
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_uuid": "ed4086d651f2f8cbed11d3c909a8873607d29a06",
    "id": "JbwfCtvKql1b"
   },
   "outputs": [],
   "source": [
    "def predict(text, include_neutral=False):\n",
    "    start_at = time.time()\n",
    "    # Tokenize text\n",
    "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
    "    # Predict\n",
    "    score = model.predict([x_test])[0]\n",
    "    # Decode sentiment\n",
    "\n",
    "    return int(round(score[0],0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_uuid": "0e920173eb05f04aecdd735bc5dff0f5be5f8d15",
    "id": "AReNBSZsql1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 876ms/step\n",
      "1/1 [==============================] - 1s 734ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_1d = []\n",
    "y_test_1d = list(df_test.label)\n",
    "scores = model.predict(x_test, verbose=1, batch_size=8000)\n",
    "y_pred_1d = [int(round(score[0],0)) for score in scores]\n",
    "\n",
    "y_pred_1d_val = []\n",
    "y_val_1d = list(df_val.label)\n",
    "scores = model.predict(x_val, verbose=1, batch_size=8000)\n",
    "y_pred_1d_val = [int(round(score[0],0)) for score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First this is an excellent kindle edition as it is true to the original and contains no scanning errors also it contains copies of the original artwork which can be zoomed to view in addition there is a glossary which contains words or phrases that may not be familiar and I found myself using the links which were underlined. Also included in the book complete with photographs is a comprehensive biography of L Ron Hubbard.The actual story is typical of the type written at the time when authors had to bash out stories in minimal time to meet editor's time scales and often paid by the word.  I have now read 4 of these books and previously thought that the \"Iron Duke\" was the weakest however I do not think this one is as good. It is a very short story probably best described as fantasy; from the cover picture I thought it was going to be a \"spaceship\" type science fiction. The story centres around a professor who discovers a formula which when allows him to instantly teleport to any destination that he thinks about or is even suggested by someone else but with little control of destination and that is about it  .Worth reading if only to get a flavour of this type of book. \n",
      "True: 1, Predicted as: 0\n",
      "201\n",
      "\n",
      "One murder is hardly a killing spree!  Give me a break people!First, let me recommend the book in this fashion:  there are five in the series, all about robots.  This first book you can get as part of the set of 5 for $2.99 (currently) or free by itself.  Get it free and see how you like it.  If you do like it, like me, then buy the series.  You'll still save money buying four more for $3 than each for $1.  I'm just saying.Now, onto the book.What you think is a typical short story about a wallflower computer programmer making a sex robot becomes much more than she bargained for.  Suddenly, after taking her robot for a spin, she's not so shy anymore and an office tryst ends badly in more than one way.  I have to wonder why she didn't turn Frank off before she went to work, that would have solved a lot of problems!I enjoyed it!  Hot.  Weird.  Shocking.  Twisted.  If you want a twisted story about an erotic robot with a screwy sense of monogamy, check this one out. \n",
      "True: 0, Predicted as: 1\n",
      "202\n",
      "\n",
      "I listened to this book through audible and was just floored.  it is a different story not the same old same old.  I read a lot and this one was totally different --the whole story with the two killers going after the &#34;bad guys&#34; and the two different POVs.  the killers and the FBI agent.  if you read alot and want a story with a different plot and story lineread this ! \n",
      "True: 0, Predicted as: 1\n",
      "203\n",
      "\n",
      "Bobby Blackhawk and Cale Yancey own and run a property outside Wellesley, Colorado that they bought together after leaving the service.Katherine Duvall or Kate as she is better known is devastated, she has been betrayed and has taken off in her rental car.When Bobby and Cale come up behind a women driving without chains on her tyres in the snow they know she is a disaster waiting to happen. Kate soon loses control of the car skidding off the road and Bobby and Cade instantly head to her rescue. What follows will surprise Kate and delight them all but will it be enough for these lone star cowboys to convince Kate to stay with them or will their winter tryst be just that.I really enjoyed Saddled, and if you love your cowboys you will too. \n",
      "True: 0, Predicted as: 1\n",
      "205\n",
      "\n",
      "I'm sure this thing rocks, except I didn't care for it as I don't have a keyboard.  It wasn't real clear on the description exactly what this thing was.  It was heavy duty and sturdy, but I don't have a keyboard to go with it.  The quality is outstanding, but it doesn't do any good if you don't have a keyboard LOL \n",
      "True: 1, Predicted as: 0\n",
      "210\n",
      "\n",
      "**May Contain Spoiler**Where to begin with Only One Love? The premise of the book is good. The idea of lost love that spans literally life times is a great concept. However, this story took a horrible turn into the unbelieveable when it went extreme paranormal and then never really elaborated on it. There was already the special ancient abilities in the Grayhawk family their really didn't need to be more. It didn't add to the story it only took away from it.The hero, Peyton, was totally different than he was in the other books of the series. In the other books he was charming, fun, reliable and in this one he was whiny and selfish. What he does to Cam is really messed up. I could've gotten past it but then he kept whining about it. I wanted to jump into the book and say \"man up!\" And don't get me started on the heroine, Avantae. She was a selfish, spineless, nymphomaniac. All she kept trying to do is get Peyton even though it would ruin his relationship with Cam. Clearly, she learned nothing from her previous lifes mistakes. She pretended to care how Cam felt but she didn't care. Then when she should have put her foot down and stood up for herself she let everyone just lead her around and make decisions for her.All the women of this series are all the same. They don't object to anything, the don't stand up for themselves, they have no self-esteem, they have to be willing to give up everything they are for these men, they have to be willing to come last when it comes to the Grayhawk family and the only thing they can expect to get from their so called \"loves\" is frequent sex, but God forbid they have any desire to be put first at any point. The men sacrifice nothing for these women the women do all the bending, all the waiting, all the being patient, all the understanding and if they ever complain along comes one of the Grayhawk siblings to tell them how beneath their family they are and how lucky they should feel that one of the Grayhawk men even looked their way so they better just accept however they're getting treated or they won't be treated at all. What the women want doesn't matter to the Grayhawk men.And talk about needing their egos stroked. This Grayhawk family need to leave the women of the world alone and just be with each other. All they want is someone to stroke their ego for them. If I had to read about how amazing one of them were again I would have thrown up. It was a little over kill on the Hawk-you're-so-great train. It was so repetitive \"No, you're the best\" \"No, you're the best\" \"No, your better than I am\" \"Oh, no, I'm nothing compared to you, Hawk,\" man, it got bad.The sad thing is I actually like the author. Her book Skin Deep was excellent I loved it, but this series needs emotionally stronger women and a little more give on the male leads part. I'm all for Alpha male but these guys don't care at all for their women. The women add nothing to their lives accept for one more person to toot their horn and are little more than glorified sex toys. All in all I'm giving up on this series but not on the author because she does have the ability to tell a good balanced story, but this series just isn't it. \n",
      "True: 1, Predicted as: 0\n",
      "211\n",
      "\n",
      "This book has a very high ICK factor.  I liked the idea of being snowbound, especially with a hero like this one.  HOWEVER, how she winds up in his bed and attracted to him is another story.  SHE BELIEVES HE IS HER BROTHER.  He has no idea but every time she is around him, attracted to him, blah, blah, blah, she BELIEVES HIM TO BE HER BROTHER.  Hiagh ick FACTOR FOR ME.  Of course, I figured that in the end he would wind up having a different father because his mother messed around but every time there is a scene of attraction everything in me is screaming HE IS YOUR BROTHER.  Now, if this doesn't bother you, you will probably like the book. But the ick factor was just too high for me to continue.  I think she could have told him, they could have figured it all out and then enjoyed the 3 days snowed in but that is just me.  Ick Ick ICK \n",
      "True: 1, Predicted as: 0\n",
      "215\n",
      "\n",
      "It's not my favorite fantasy read and it'd probably be much better with lots of editing. Very hard to follow as it's so overwritten. Adverbs abound. Shoulders aren't just muscled they're lumpily muscled shoulders. \"Aglow in the bright radiance of the system's young yellow sun, Level One was the shining face the Alliance presented to the galaxy.\" \"was hanging from a strut overhead by his long arms, impatiently digging his huge fingers into the support hard enough to dimple the metal.\" \"The underlying shape, however, was almost invisible under a multitude of sensor spars, invaluable in scanning for salvage.\" \"She hurried furtively down the street, scanning side to side through the shadows.\" \"There was blessed release in that gun, in the cold heart of the man who wielded it.\" \n",
      "True: 1, Predicted as: 0\n",
      "219\n",
      "\n",
      "Not worth buying and definitely not worth the time or effort reading.  This was a disappointment because I had read other works by this author. \n",
      "True: 1, Predicted as: 0\n",
      "243\n",
      "\n",
      "First, let me say that I enjoyed this book and the angle that the author took. However, due to the way that it is written, I did not develop any kind of attachment to any of the main characters. This book is basically written as a narrative of events that happened, reminding me of a fictional history book. The book also gets very technical about its development of the space drive and weapons.  Liked it, didn't love it. Will buy the next one to see what happens. \n",
      "True: 0, Predicted as: 1\n",
      "251\n",
      "\n",
      "I love all of Lisa Marie Rice's  books, except this one, which I hated it.  The hero is beyond abnoxious.  I detest when the hero has commitment issues until the end, especiallly when the heroine is so excited about the possibility of a relationship with the hero early on. \n",
      "True: 1, Predicted as: 0\n",
      "254\n",
      "\n",
      "Nothing new, nothing particularly interesting. Your average contract killer and all the associated gimmicks.Not a particularly clever plot, although always aiming at intelligence.Not worth the time spent, seriously. \n",
      "True: 1, Predicted as: 0\n",
      "258\n",
      "\n",
      "This was a great book for a supe short story. The way things start out with a great plot and setting with characters that are just out there in a way that you have no chocie but to continue reading.Everyone had a place in this story. I like the way he dealt with the theft. I must say that it was very unusual how the twist of events came about, but not to tell the story all I can say is it is a must read and you will be suprised. \n",
      "True: 1, Predicted as: 0\n",
      "275\n",
      "\n",
      "Had no idea this author existed until this week, sad I know since I've read hundreds of IR titles over the years.  Well, I guess it's better late than never.  I really, really enjoyed this book.  Unlike most IR/MC this book was well-written, sharply edited and insanely funny just how I like. I would recommend:) \n",
      "True: 0, Predicted as: 1\n",
      "276\n",
      "\n",
      "I applied myself to this after reading RA Salvatore and some general classics and well if they turned episodes of the clones wars into books, then this wouldn;t stand a chance. Sith Crash landing... and then the establishment of characters which didn't gain (me) any attachment. I didn;t care if they lived or died.it would have been more interesting to see Jedi crash landing and through necessity they turn into Sith. \n",
      "True: 1, Predicted as: 0\n",
      "277\n",
      "\n",
      "I like old-fashioned westerns and I like to read those by Zane Grey. He is a particularly good writer, and many of his rip-roarin' westerns have rightly become classics. But be aware a goodly number of his books are more \"prairie romance\" than action-packed western, and in these there's often more huff and bluff than gunfire. Many of his books were not set in the \"old west\" of the past but in \"modern\" times, meaning contemporary to when he wrote them; these often depicted a rugged male abiding by a highly developed sense of fair-play and moral integrity (in contrast to the sneaky \"villain\" who did not), a chaste female heroine, misunderstanding, romance, and overall wholesomeness leading to a happy ending. Several works in this collection--though they  may be set in the western states--are not what we would normally call \"westerns,\" and others are baseball stories and nonfiction accounts of fishing and camping. Sadly the majority of his greatest westerns are not yet inexpensively or freely available in public domain editions. No matter what he wrote (prairie romance, baseball stories, fishing accounts, or classic westerns), Grey knew how to tell a good story, and his vivid descriptions are second to none.UPDATE 05-08-13: You may wish to compare this set of 26 to another Zane Grey collection recently published by Wildside Press as \"The Zane Grey Megapack.\" Attractively formatted and offering 42 novels and stories for only $.99, that now seems like the better deal. \n",
      "True: 0, Predicted as: 1\n",
      "279\n",
      "\n",
      "This book really suprised me. This started out good and ended really well.i can't remember any other books I've read from this author, but now I do intend to read more. \n",
      "True: 1, Predicted as: 0\n",
      "283\n",
      "\n",
      "After signing up for some free kindle book, I have a hodgepodge of reading material.  My usual thing is horror.  This book was fast paced and very interesting.  I sorta saw where it was going, but the end was still a surprise. \n",
      "True: 0, Predicted as: 1\n",
      "285\n",
      "\n",
      "Colters' Wife (Colter's Legacy #1.5) by Maya Banks is a sequel to Maya's most popular book, Colters' Woman where three brothers all share the same woman and end up \"marrying\" her. I haven't read Colters' Woman since I'm not a big fan of siblings involved in a polyamorous incestuous relationship regardless if the siblings aren't sexually attracted to one another. In Colters' Wife, each Colter brother has one-on-one time with the pregnant heroine, Holly. There's no threesomes or quartet loving going on here.Colters' Wife goes as follows: Pregnant woman has sex with three brothers at different times, falls down a ditch, gives birth, The End. The sex was hot but there really wasn't much of a plot.When Holly fell down the ditch, I was expecting some Lassie type dog to run and get help. Unfortunately, no lovable dog appeared, and because of this I had to lower the grade because nothing says happily ever after like a woman married to three brothers and a pet dog they can all love and cherish. But then the dog wouldn't be able to sleep in their bed because there's no room with three brawny men and one small woman sleeping together.If you're a fan of Maya's Coulter series, you'll enjoy this short. It's both cheesy and equally disturbing having three brothers sharing one woman. Holly must be exhausted from all the Colter loving she gets. But then again Holly doesn't have any complaints because the triple threat Coulter men are too perfect for words and have all the right moves.Katiebabs \n",
      "True: 1, Predicted as: 0\n",
      "286\n",
      "\n",
      "I stopped reading in chapter one.  I purely hated what little I read and will most likely by-pass the books of this author.  There are just some things I can't quite do. \n",
      "True: 1, Predicted as: 0\n",
      "287\n",
      "\n",
      "I didn't care too much for this book. It didn't grab me like the other books I've read by Lisa Jackson \n",
      "True: 1, Predicted as: 0\n",
      "290\n",
      "\n",
      "While the descriptions were vivid, the story lacked any interesting characters. I also find referring to zombies as &#34;deaders&#34; quite obnoxious. Just because &#34;The Walking Dead&#34; came up with a succinct way to describe them doesn't mean you can just throw words out there and assume they'll work. \n",
      "True: 1, Predicted as: 0\n",
      "292\n",
      "\n",
      "I wanted to like this, but the spark seemed to be missing.Setting aside whether or not something like this really could have led to the feud between the families, this fell short of the intended goal of an erotic story. \n",
      "True: 1, Predicted as: 0\n",
      "294\n",
      "\n",
      "Aggie proves once again that an observant eye and brains can overcome an aging body and a criminal mind.A little more editing for improper word usage, spelling errors and grammar, however, is in order. \n",
      "True: 0, Predicted as: 1\n",
      "298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing some examples of sentences our model got wrong from the validation data. \n",
    "val  = pd.read_csv(validation_domain)\n",
    "for i in range(200,300):\n",
    "    pred = y_val_1d[i]\n",
    "    true = y_pred_1d_val[i]\n",
    "    if pred != true:\n",
    "        print(val['text'][i],f'\\nTrue: {true}, Predicted as: {pred}\\n{i}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e23b957348dcc084249d3cc7538b972da471c2cd",
    "id": "YBErX3ebql1c"
   },
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_uuid": "a7fe05b7caa1c984ff1deb0be2f7c6bc043df9f5",
    "id": "gQqva0fRql1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for predicting on validation data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.64      0.69       382\n",
      "           1       0.80      0.87      0.83       618\n",
      "\n",
      "    accuracy                           0.78      1000\n",
      "   macro avg       0.78      0.76      0.76      1000\n",
      "weighted avg       0.78      0.78      0.78      1000\n",
      " \n",
      "\n",
      "\n",
      "Classification report for predicting on test data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.56      0.68       514\n",
      "           1       0.66      0.90      0.76       486\n",
      "\n",
      "    accuracy                           0.72      1000\n",
      "   macro avg       0.75      0.73      0.72      1000\n",
      "weighted avg       0.76      0.72      0.72      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing classification reports for validation and test data:\n",
    "\n",
    "print('Classification report for predicting on validation data:\\n',classification_report(y_val_1d, y_pred_1d_val),'\\n\\n')\n",
    "\n",
    "print('Classification report for predicting on test data:\\n',classification_report(y_test_1d, y_pred_1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f014c32f3833db282e1a075c526604f34e3158c",
    "id": "eoK2fknEql1d"
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "3b2b3ad5b592977b404acfa1c9ad303a62837255",
    "id": "RYoPqa7kql1d"
   },
   "outputs": [],
   "source": [
    "if not pretrained_model:\n",
    "    model.save(KERAS_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc363c54782894757f5ea8820c6a170f2e16ef93",
    "id": "A_FOlOf-ql1d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Kopi af LSTM-model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
